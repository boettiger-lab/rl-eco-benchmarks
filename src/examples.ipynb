{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples for how to use our ecological-RL API\n",
    "\n",
    "Our approach seeks to minimize the amount of computational information that the user\n",
    "needs to provide in order to get an RL algorithm up and running on their population\n",
    "dynamics control problem.\n",
    "\n",
    "## 1. Using ray RLLib to train\n",
    "\n",
    "The class `ray_trainer_api.ray_trainer` may be used for defining, tuning, and training an agent using the ray RLLib framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install ray[rllib]\n",
    "#! pip install gymnasium\n",
    "#! pip install numpy\n",
    "#! pip install pandas\n",
    "#! pip install scipy\n",
    "\n",
    "from ray_trainer_api import ray_trainer\n",
    "from dyn_fns import threeSp_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecological input\n",
    "\n",
    "The cell below translates the ecological data defining the control problem to the format that our classes use.\n",
    "\n",
    "The `metadata` dictionary encapsulates most of the information of the control problem, except for the actual dynamics of the system. `dyn_fn` encapsulates the dynamics of the system (note that the number of arguments of this function must match `metadata['n_sp']`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "\t#\n",
    "\t# structure of ctrl problem\n",
    "\t'name': 'threeSp_1',\n",
    "\t'n_sp':  3,\n",
    "\t'n_act': 2,\n",
    "\t'_harvested_sp': [0,1],\n",
    "\t#\n",
    "\t# about episodes\n",
    "\t'init_pop': np.float32([0.5, 0.5, 0.1]),\n",
    "\t'reset_sigma': 0.01,\n",
    "\t'tmax': 1000,\n",
    "\t#\n",
    "\t# about dynamics / control\n",
    "    'extinct_thresh': 0.05,\n",
    "    'penalty_fn': lambda t: - 1000 / t,\n",
    "\t'var_bound': 4,\n",
    "\t'_costs': np.zeros(2, dtype=np.float32),\n",
    "\t'_prices': np.ones(2, dtype=np.float32),\n",
    "}\n",
    "\n",
    "params = {\n",
    "\t'c': np.random.choice([0.2, 0.25, 0.3]),\n",
    "\t'D': np.random.choice([0.05, 0.1, 0.15]),\n",
    "\t'd_z': np.random.choice([0.2, 0.3, 0.4]),\n",
    "\t'K_x': np.random.choice([0.9, 1, 1.1]),\n",
    "\t'LV_xy': np.random.choice([0.05, 0.1, 0.15]),\n",
    "\t'r_x': np.random.choice([0.9, 1, 1.1]),\n",
    "\t'r_y': np.random.choice([0.9, 1, 1.1]),\n",
    "\t'r_z': np.random.choice([0.9, 1, 1.1]),\n",
    "\t#\n",
    "\t'sigma_x': 0.1,\n",
    "\t'sigma_y': 0.1,\n",
    "\t'sigma_z': 0.1,\n",
    "}\n",
    "\n",
    "def dyn_fn(x,y,z):\n",
    "\tglobal params\n",
    "\treturn threeSp_1(x,y,z,_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "With the previous setup, we may define our trainer and train it as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT = ray_trainer(\n",
    "\talgo_name=\"ppo\", \n",
    "\tconfig={\n",
    "        'metadata': metadata,\n",
    "        'dyn_fn': dyn_fn,\n",
    "    },\n",
    ")\n",
    "agent = RT.train(iterations=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
