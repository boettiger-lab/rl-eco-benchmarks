{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples for how to use our ecological-RL API\n",
    "\n",
    "Our approach seeks to minimize the amount of computational information that the user\n",
    "needs to provide in order to get an RL algorithm up and running on their population\n",
    "dynamics control problem.\n",
    "\n",
    "## 1. Using ray RLLib to train\n",
    "\n",
    "The class `ray_trainer_api.ray_trainer` may be used for defining, tuning, and training an agent using the ray RLLib framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# necessary installations for our package:\n",
    "\n",
    "#! pip install ray[rllib]\n",
    "#! pip install gymnasium\n",
    "#! pip install numpy\n",
    "#! pip install pandas\n",
    "#! pip install scipy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ray_trainer_api import ray_trainer\n",
    "from dyn_fns import threeSp_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecological input\n",
    "\n",
    "The cell below translates the ecological data defining the control problem to the format that our classes use.\n",
    "\n",
    "The `metadata` dictionary encapsulates most of the information of the control problem, except for the actual dynamics of the system. `dyn_fn` encapsulates the dynamics of the system (note that the number of arguments of this function must match `metadata['n_sp']`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "\t#\n",
    "\t# structure of ctrl problem\n",
    "\t'name': 'threeSp_1',\n",
    "\t'n_sp':  3,\n",
    "\t'n_act': 2,\n",
    "\t'_harvested_sp': [0,1],\n",
    "\t#\n",
    "\t# about episodes\n",
    "\t'init_pop': np.float32([0.5, 0.5, 0.1]),\n",
    "\t'reset_sigma': 0.01,\n",
    "\t'tmax': 1000,\n",
    "\t#\n",
    "\t# about dynamics / control\n",
    "    'extinct_thresh': 0.05,\n",
    "    'penalty_fn': lambda t: - 1000 / (t+1),\n",
    "\t'var_bound': 4,\n",
    "\t'_costs': np.zeros(2, dtype=np.float32),\n",
    "\t'_prices': np.ones(2, dtype=np.float32),\n",
    "}\n",
    "\n",
    "params = {\n",
    "\t'c': np.random.choice([0.2, 0.25, 0.3]),\n",
    "\t'D': np.random.choice([0.05, 0.1, 0.15]),\n",
    "\t'd_z': np.random.choice([0.2, 0.3, 0.4]),\n",
    "\t'K_x': np.random.choice([0.9, 1, 1.1]),\n",
    "\t'LV_xy': np.random.choice([0.05, 0.1, 0.15]),\n",
    "\t'r_x': np.random.choice([0.9, 1, 1.1]),\n",
    "\t'r_y': np.random.choice([0.9, 1, 1.1]),\n",
    "\t'r_z': np.random.choice([0.9, 1, 1.1]),\n",
    "\t#\n",
    "\t'sigma_x': 0.1,\n",
    "\t'sigma_y': 0.1,\n",
    "\t'sigma_z': 0.1,\n",
    "}\n",
    "\n",
    "def dyn_fn(x,y,z):\n",
    "\tglobal params\n",
    "\treturn threeSp_1(x, y, z, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "With the previous setup, we may define our trainer and train it as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 18:15:29,498\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-08-22 18:15:29,500\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=5945)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "2023-08-22 18:15:32,634\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5945)\u001b[0m 2023-08-22 18:15:32,589\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5944)\u001b[0m 2023-08-22 18:15:32,609\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5944)\u001b[0m 2023-08-22 18:15:32,609\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5944)\u001b[0m 2023-08-22 18:15:32,609\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5944)\u001b[0m 2023-08-22 18:15:32,609\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-08-22 18:15:32,683\tWARNING util.py:68 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration nr. 1\r"
     ]
    }
   ],
   "source": [
    "RT = ray_trainer(\n",
    "\talgo_name=\"ppo\", \n",
    "\tconfig={\n",
    "        'metadata': metadata,\n",
    "        'dyn_fn': dyn_fn,\n",
    "    },\n",
    ")\n",
    "agent = RT.train(iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
